# config.toml

[data]
test_data_path = "data/heart_big_test.parq"                                               # Pad naar je trainingsdata (relatief ten opzichte van de project root)
train_data_path = ["data/heart_big_train.parq", "data/heart_big_train_synthetic.parquet"] # Pad naar je testdata (relatief ten opzichte van de project root)

[training]
batch_size = 32        # Grootte van de batches die aan het model worden gevoerd
epochs = 100           # Aantal trainings-epochs (passes over de volledige dataset)
learning_rate = 0.001  # InitiÃ«le learning rate voor de optimizer
optimizer = "Adam"     # Type optimizer om te gebruiken (bijv. "Adam", "SGD")
scheduler_factor = 0.1 # Factor waarmee de learning rate wordt verminderd door de scheduler (bijv. 0.1 voor 10x reductie)
scheduler_patience = 3 # Aantal epochs zonder verbetering voordat de learning rate wordt aangepast
log_dir = "runs"       # Directory waar trainingslogs en modelartefacten worden opgeslagen

early_stopping_patience = 20         # Aantal epochs om te wachten zonder verbetering van testloss
early_stopping_min_delta = 0.001     # Minimale verbetering in testloss om als 'verbetering' te tellen
use_scheduler = true                 # Zet deze op 'true' om de scheduler in te schakelen
scheduler_type = "ReduceLROnPlateau" # Of "StepLR", afhankelijk van je voorkeur
weight_decay = 0.0005                # Voorbeeld van weight decay (L2 regularisatie)

[training.scheduler_kwargs] 
mode = "min" # Voor ReduceLROnPlateau: 'min' voor monitoring van verlies, 'max' voor monitoring van metrics (bijv. accuracy)
factor = 0.1 # De factor waarmee de leerfrequentie wordt verminderd (nieuwe_lr = oude_lr * factor)
patience = 5 # Aantal epochs zonder verbetering voordat de leerfrequentie wordt verminderd

# --- Model Specifieke Parameters ---
# Let op: de 'input_size_after_flattening' voor de CNN wordt dynamisch berekend in main.py.
# De parameters in deze secties dienen als standaardwaarden.
# De 'hyper_params_list' in main.py overschrijft deze voor specifieke experimentruns.

[model_params.baseline]
input_size = 187 # Input grootte van het baseline model (aantal features)
output_size = 5  # Output grootte van het baseline model (aantal klassen)

[model_params.cnn]
input_channels = 1      # Aantal input kanalen (1 voor 1D numerieke features, zoals een tijdreeks of platte features)
hidden_size = 64        # Grootte van de volledig verbonden (dense) verborgen laag
num_layers = 2          # Aantal convolutionele lagen in het model # Deze heeft meer betrekking op de diepte van de CNN-blokken
output_size = 5         # Output grootte van het CNN model (aantal klassen)
conv_filters = [32, 64] # Lijst van het aantal filters in elke convolutionele laag
kernel_size = 3         # Grootte van de convolutionele kernel
use_dropout = true      # Boolean om aan te geven of dropout gebruikt moet worden
dropout_rate = 0.4      # Dropout rate indien use_dropout = true

[model_params.gru]
input_size = 187  # Input grootte van het GRU model (aantal features per tijdstap)
hidden_size = 128 # Grootte van de verborgen toestand (hidden state) van de GRU-cellen
num_layers = 2    # Aantal gestapelde GRU-lagen
output_size = 5   # Output grootte van het GRU model (aantal klassen)
dropout = 0.2     # Dropout rate toegepast op de output van elke GRU-laag (behalve de laatste)

[model_params.cnn_se_skip]
input_channels = 1          # Aantal input kanalen (1 voor 1D numerieke features)
hidden_size = 64            # Grootte van de volledig verbonden (dense) verborgen laag na de laatste conv block
num_layers = 2              # Aantal herhalende CNN-SE-Skip blokken
output_size = 5             # Output grootte van het model (aantal klassen)
conv_filters_per_block = 32 # Aantal filters in elke convolutionele laag binnen een blok
kernel_size_per_block = 3   # Grootte van de convolutionele kernel binnen een blok
use_dropout = true          # Boolean om aan te geven of dropout gebruikt moet worden
dropout_rate = 0.3          # Dropout rate indien use_dropout = true
reduction_ratio = 8         # Reductiefactor voor het Squeeze-and-Excitation blok
